{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk, re, string\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re, collections\n",
    "from nltk.corpus import words as w\n",
    "import pickle\n",
    "from nltk.corpus import *\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocess data, tokenize and lemmatize\n",
    "2. Feature extraction - TF-IDF, word2vec and custom features\n",
    "3. Train Linear SVM Model\n",
    "4. Fit data and check accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv file and make a dataframe.\n",
    "- For training: Randomize and Divide it into 80:20 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_Dataset(run=\"train\"):\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    df = df[df[\"Comment\"].notnull()]\n",
    "    df.apply(np.random.permutation)\n",
    "    if run==\"train\":\n",
    "        df_train = df[:round(0.8*len(df))]\n",
    "        df_test = df[round(0.8*len(df)):]\n",
    "    elif run==\"test\":\n",
    "        df_train = df\n",
    "        df_test = pd.read_csv(\"test_with_solutions.csv\")\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 (Did not work)\n",
    "In addition to tokenizion, the spell check mechanism corrects the following:\n",
    "1. Extra letters added in a word which need to be deleted.\n",
    "2. Transpose of letters within words.\n",
    "3. Replacement of certain letters for another letter altogether.\n",
    "4. Inserting letters that may have been missed in the word.\n",
    "5. Handling repeated letters to get the correct spelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def words(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1   \n",
    "    return model\n",
    "\n",
    "with open(\"big.txt\", \"r\") as big:\n",
    "    word_corpus = big.read()\n",
    "for word in w.words():\n",
    "    word_corpus += word\n",
    "    \n",
    "\n",
    "NWORDS = train(words(word_corpus))\n",
    "with open(\"list_of_abuses.txt\", \"r\") as abuse_list:\n",
    "    abuses = abuse_list.read().split()\n",
    "    for abuse in abuses:\n",
    "        NWORDS[abuse] = 100\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "#     print(word)\n",
    "    s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in s if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in s if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in s for c in alphabet if b]\n",
    "    inserts    = [a + c + b     for a, b in s for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words):\n",
    "    try:\n",
    "        return [int(w) for w in words] #to take care of purely numeric words\n",
    "    except:\n",
    "        return set(w for w in words if w.lower() in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    if word[0] not in alphabet: \n",
    "        return word\n",
    "    else:\n",
    "        word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "        candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "        return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w!@#$%^&*]+)', # To group symbols together\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    \n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    tokens = tokens_re.findall(s)\n",
    "    for i in range(len(tokens)):\n",
    "        clean_token = correct(tokens[i])\n",
    "        tokens[i] = clean_token\n",
    "    return tokens\n",
    " \n",
    "def preprocess(word, lowercase=False):\n",
    "    tokens = tokenize(word)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a text and does the following to return the tokens:\n",
    "* Use nltk's TweetTokenizer to get tokens\n",
    "* Use wordNetLemmatizer for lemmatization\n",
    "* Use porterStemmer to stem the resulting tokens (Did not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_tokens(text):\n",
    "    tweetTokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tokens = tweetTokenizer.tokenize(text)\n",
    "    #tokens = preprocess(text, lowercase=True)\n",
    "    tokens = [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "    #tokens= [nltk.PorterStemmer().stem(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What didn't work\n",
    "* Stemming and custom preprocessing reduced the accuracy. Tweettokenizer worked better. We also realized that making the tweets into lower case resulted in loss of emphasis and ended up preserving the case of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Features\n",
    "We added the below custom features:\n",
    "* Percentage of bad words in a sentence as listed in the bad words file\n",
    "* Compound, negative and positive values from vader sentiment analysis\n",
    "* Insults directed at a person defined as 'you are <bad-word>' and if the sentence doesnt contain 'not'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdditionalFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return (['vader_compound','vader_neg','vader_pos','vader_neu','ur_bad'])\n",
    "    \n",
    "    def num_bad(self, df):\n",
    "       \n",
    "        #get number of words in each sentence\n",
    "        with open(\"list_of_abuses.txt\", \"r\") as abuse_list:\n",
    "             abuses = abuse_list.read().split(\"\\n\")\n",
    "        num_words = [len(word) for word in df]\n",
    "        \n",
    "        #get percent of abusive words in each sentence\n",
    "        num_abuses = 0\n",
    "        for abuse in abuses:\n",
    "            num_abuses += 1\n",
    "        # number of badwords in list of abuses\n",
    "        num_bad = [np.sum([word.lower().count(abuse) for abuse in abuses])\n",
    "                                            for word in df]\n",
    "        norm_bad = np.array(num_bad) / np.array(num_words, dtype=np.float)\n",
    "        return norm_bad\n",
    "\n",
    "    def ur_bad(self, df):\n",
    "        \n",
    "        #look for you are \"bad words\" in text\n",
    "        with open(\"list_of_abuses.txt\", \"r\") as abuse_list:\n",
    "             abuses = abuse_list.read().split(\"\\n\")\n",
    "        ur_bad_bool = []\n",
    "        for sentence in df:\n",
    "            abuse_found = False\n",
    "            for words in abuses:\n",
    "                abuse = r'(\\b%s\\b)' %words\n",
    "                if (re.findall(r\"\\byou are\\b\",sentence.lower()) != []) and (re.findall(abuse,sentence.lower()) != []):\n",
    "                    if (re.findall(r\"\\bnot\\b\",sentence.lower()) == []):\n",
    "                        ur_bad_bool.append(True)\n",
    "                        abuse_found = True\n",
    "                        break\n",
    "                elif (re.findall(r\"\\bur\\b\",sentence.lower()) != []) and (re.findall(abuse,sentence.lower()) != []):\n",
    "                    if (re.findall(r\"\\bnot\\b\",sentence.lower()) == []):\n",
    "                        ur_bad_bool.append(True)\n",
    "                        abuse_found = True\n",
    "                        break\n",
    "                elif (re.findall(r\"\\byour\\b\",sentence.lower()) != []) and (re.findall(abuse,sentence.lower()) != []):\n",
    "                    if (re.findall(r\"\\bnot\\b\",sentence.lower()) == []):\n",
    "                        ur_bad_bool.append(True)\n",
    "                        abuse_found = True\n",
    "                        break\n",
    "                elif (re.findall(r\"\\byou're\\b\",sentence.lower()) != []) and (re.findall(abuse,sentence.lower()) != []):\n",
    "                    if (re.findall(r\"\\bnot\\b\",sentence.lower()) == []):\n",
    "                        ur_bad_bool.append(True)\n",
    "                        abuse_found = True\n",
    "                        break\n",
    "                elif (re.findall(r\"\\bu r\\b\",sentence.lower()) != []) and (re.findall(abuse,sentence.lower()) != []):\n",
    "                    if (re.findall(r\"\\bnot\\b\",sentence.lower()) == []):\n",
    "                        ur_bad_bool.append(True)\n",
    "                        abuse_found = True\n",
    "                        break\n",
    "            if abuse_found == False:\n",
    "                ur_bad_bool.append(False)            \n",
    "        return ur_bad_bool\n",
    "    \n",
    "    def num_words(self, df):\n",
    "        \n",
    "        #get number of words in each sentence\n",
    "        num_words = [len(word) for word in df]\n",
    "        return num_words\n",
    "    \n",
    "    def vader_helper(self, df):\n",
    "        \n",
    "        #vader analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_feature = []\n",
    "        for sentence in df:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            vader_feature.append(ss['compound'])\n",
    "        return vader_feature\n",
    "    \n",
    "    def vader_helper_neg(self, df):\n",
    "        \n",
    "        #vader analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_feature = []\n",
    "        for sentence in df:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            vader_feature.append(ss['neg'])\n",
    "        return vader_feature\n",
    "    \n",
    "    def vader_helper_neu(self, df):\n",
    "       \n",
    "        #vader analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_feature = []\n",
    "        for sentence in df:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            vader_feature.append(ss['neu'])\n",
    "        return vader_feature\n",
    "    \n",
    "    def vader_helper_pos(self, df):\n",
    "        #vader analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_feature = []\n",
    "        for sentence in df:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            vader_feature.append(ss['pos'])\n",
    "        return vader_feature\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        \n",
    "        #add all the features to an array\n",
    "        X = np.array([self.vader_helper(df),self.vader_helper_pos(df),self.vader_helper_neg(df),self.vader_helper_neu(df),self.ur_bad(df)]).T\n",
    "        if not hasattr(self, 'scalar'):\n",
    "            self.scalar = preprocessing.StandardScaler().fit(X)\n",
    "        return self.scalar.transform(X)      \n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Features\n",
    "* To make Tweet Vector we take all tokens in a tweet and see if we have a word2vec for that word. We keep adding all these word vectors and take average to get a tweet vector\n",
    "* We are making our entire tweet into a 200 dimensional vector\n",
    "* We take our tfidf and word vectors and append them to get the final vector. Since these are of different scale, we are normalising the final vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return (['word2vec'])\n",
    "    \n",
    "    def make_tweetVector(self, df_comments):\n",
    "        learned_word_vectors,learned_dictionary = pickle.load( open( \"glove_twitter_200d.pkl\", \"rb\" ) ) \n",
    "        tweetVector = np.zeros((len(df_comments),200),dtype='float')\n",
    "        count = 0 \n",
    "        for text in df_comments:\n",
    "            valids= 0\n",
    "            tokens = build_tokens(text)\n",
    "            rowVector = np.zeros(200,)\n",
    "            for token in tokens:\n",
    "                if token not in learned_dictionary:\n",
    "                    continue\n",
    "                else:\n",
    "                    valids+=1\n",
    "                    vec = learned_word_vectors[learned_dictionary[token.lower()]]\n",
    "                    rowVector = np.add(rowVector,vec)\n",
    "            tweetVector[count] = rowVector/valids\n",
    "            count+=1\n",
    "        return tweetVector\n",
    "    \n",
    "    def transform(self, df, y=None):     \n",
    "        #add both the features to an array\n",
    "        X = np.array(self.make_tweetVector(df))\n",
    "        if not hasattr(self, 'scalar'):\n",
    "            self.scalar = preprocessing.StandardScaler().fit(X)\n",
    "        return self.scalar.transform(X)      \n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Union\n",
    "* Most of the stop words could influence our results, so we removed only the articles. We found that considering 6-grams gave the best accuracy.\n",
    "* The custom features and Word2Vec features are stacked along with the features got from TF-IDF char and word analyzer. \n",
    "* Using all the features rather than selected number of features gave a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = ['the','a','an']\n",
    "def all_features():\n",
    "    features = []\n",
    "    custom_features = AdditionalFeatureExtractor() # this class includes my custom features \n",
    "    \n",
    "    vect = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,6), analyzer= \"char\", stop_words = stopwords, tokenizer= build_tokens)\n",
    "    vect1 = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,6), analyzer= \"word\",stop_words = stopwords, tokenizer= build_tokens)\n",
    "    word2vec = Word2VecFeatureExtractor()\n",
    "    features.append(('ngram', vect))\n",
    "    features.append(('ngram1', vect1))\n",
    "    features.append(('word2vec',word2vec))\n",
    "    features.append(('custom_features', custom_features))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n",
    "* SVM Linear classifier gives the best score - better than ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_classifiers():\n",
    "    #clf1 = linear_model.LogisticRegression(C=3, max_iter=3000, tol=1e-8)\n",
    "    clf3 = svm.SVC(kernel='linear', gamma=1.2, C=1, decision_function_shape=\"ovo\",probability=True)\n",
    "    #clf4 = linear_model.SGDClassifier(n_iter=2000,loss = 'modified_huber', penalty = 'elasticnet',alpha=0.001, n_jobs=-1)\n",
    "    #eclf = VotingClassifier(estimators=[('lr',clf1),('svm_rbf',clf3), ('sgd' , clf4)], voting=\"soft\")\n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "* The pipeline calls the feature function and creates a featureunion of all the features discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "best_clf = Pipeline([\n",
    "    ('all', FeatureUnion(all_features())),\n",
    "    ('linear',all_classifiers()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit data and check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, df_test = load_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x110e4ae48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFdCAYAAACTqR4KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAADpJJREFUeJzt3V2M5WddwPHvIwXXYkBhtI3R1Zr6MiZG2QXfURQTEowY\nY4JZuyGGC0PQCxsTjBcGo3caRKPWGI2vCxvfDRdIBWJ8BQldIaJDNVhEwVYXyWqE+tL+vTjP6nRp\nizud85/Z3c8nORdzzpl5nvnNf2e+c87/7IxlWQIA+Lij3gAAcDyIAgCgEgUAwCQKAIBKFAAAkygA\nACpRAABMN23zg48xnlm9oHpv9eA21wKA68yJ6rOqu5dl+eAaC241CtoEwWu2vAYAXM/uqF67xkLb\njoL3Vp07d67d3d0tL8Vld955Z69+9auPehs3FDNfn5mvz8zXtbe319mzZ2v+LF3DtqPgward3d1O\nnTq15aW47OlPf7p5r8zM12fm6zPzI7Pa0+9ONAQAKlEAAEyiAACoRMF16cyZM0e9hRuOma/PzNdn\n5te/sSzL9j74GKeqe+655x4npwDAVbhw4UKnT5+uOr0sy4U11vRIAQBQiQIAYBIFAEAlCgCASRQA\nAJUoAAAmUQAAVKIAAJhEAQBQiQIAYBIFAEAlCgCASRQAAJUoAAAmUQAAVKIAAJhEAQBQiQIAYBIF\nAEAlCgCASRQAAJUoAAAmUQAAVKIAAJhEAQBQiQIAYBIFAEAlCgCASRQAAJUoAACmm9ZYZG9vb41l\nAODQ7OzsdPLkyaPexqpWiYKzZ8+usQwAHJoTJ27u3nv3bqgwWCUK6oeqF66zFAA8YXs9+ODZLl68\nKAoO323VqXWWAgAOxImGAEAlCgCASRQAAJUoAAAmUQAAVKIAAJhEAQBQiQIAYBIFAEAlCgCASRQA\nAJUoAAAmUQAAVKIAAJhEAQBQiQIAYBIFAEAlCgCASRQAAJUoAAAmUQAAVKIAAJhEAQBQiQIAYBIF\nAEAlCgCASRQAAJUoAAAmUQAAVKIAAJhEAQBQHTAKxhjfOca4b4zxkTHGW8cYzznsjQEA67rqKBhj\nfGv1quqV1bOqd1Z3jzF2DnlvAMCKDvJIwZ3VzyzL8svLsry7eln14eqlh7ozAGBVVxUFY4wnV6er\nN1++blmWpXpT9eWHuzUAYE1X+0jBTvWk6oErrn+guvVQdgQAHImb1lnmVdWvXnHdmXkBgBvb+fPn\nO3/+/COuu3Tp0ur7uNoouFg9VN1yxfW3VPc/9rt9T3XHVS4FADeGM2fOdObMI39RvnDhQqdPn151\nH1f19MGyLP9V3VM9//J1Y4wx3/7Tw90aALCmgzx98KPVL44x7qne1ubVCDdXv3iI+wIAVnbVUbAs\ny6/N/5PgB9s8bfCO6gXLsvzzYW8OAFjPgU40XJblruquQ94LAHCE/O0DAKASBQDAJAoAgEoUAACT\nKAAAKlEAAEyiAACoRAEAMIkCAKASBQDAJAoAgEoUAACTKAAAKlEAAEyiAACoRAEAMIkCAKASBQDA\nJAoAgEoUAACTKAAAKlEAAEyiAACoRAEAMIkCAKASBQDAJAoAgEoUAACTKAAAKlEAAEyiAACoRAEA\nMN20zjL3VRfWWQoAnrC9o97AkVgpCr5/XgDg2nDixM3t7Owc9TZWtUoUnDt3rt3d3TWWAoBDsbOz\n08mTJ496G6taJQp2d3c7derUGksBAAfkREMAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAw\niQIAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAwiQIAoBIFAMAkCgCAShQAAJMoAAAqUQAA\nTKIAAKhEAQAwiQIAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAwiQIAoBIFAMAkCgCAShQA\nAJMoAAAqUQAATKIAAKhEAQAwiQIAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAwiQIAoBIF\nAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAwiQIAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhE\nAQAwiQIAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAwiQIAoKqb1lhkb29vjWWgnZ2dTp48\nedTbALgmrRIFZ8+eXWMZ6MSJm7v33j1hAHAAq0RB/VD1wnWW4ga214MPnu3ixYuiAOAAVoqC26pT\n6ywFAByIEw0BgEoUAACTKAAAKlEAAEyiAACoRAEAMIkCAKASBQDAJAoAgEoUAACTKAAAKlEAAEyi\nAACoRAEAMIkCAKASBQDAJAoAgEoUAACTKAAAKlEAAEyiAACoRAEAMIkCAKASBQDAJAoAgEoUAACT\nKAAAKlEAAEyiAACoRAEAMIkCAKA6QBSMMZ47xnjdGOP9Y4yHxxgv2sbGAIB1HeSRgqdW76heXi2H\nux0A4KjcdLXvsCzLG6o3VI0xxqHvCAA4Es4pAAAqUQAATFf99MHBvKr61SuuOzMvAHBjO3/+fOfP\nn3/EdZcuXVp9HytFwfdUd6yzFABcY86cOdOZM4/8RfnChQudPn161X14+gAAqA7wSMEY46nV7dXl\nVx589hjji6p/WZbl7w9zcwDAeg7y9MGzq99v838ULG1OGKj6peqlh7QvAGBlB/l/Cv4gTzsAwHXH\nD3cAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAwiQIAoBIFAMAkCgCAShQAAJMoAAAqUQAA\nTKIAAKhEAQAwiQIAoBIFAMAkCgCAShQAAJMoAAAqUQAATKIAAKhEAQAwiQIAoBIFAMAkCgCAShQA\nAJMoAAAqUQAATKIAAKhEAQAw3bTOMvdVF9ZZihvY3lFvAOCatlIUfP+8wHadOHFzOzs7R70NgGvS\nKlFw7ty5dnd311iKG9zOzk4nT5486m0AXJNWiYLd3d1OnTq1xlIAwAE50RAAqEQBADCJAgCgEgUA\nwCQKAIBKFAAAkygAACpRAABMogAAqEQBADCJAgCgEgUAwCQKAIBKFAAAkygAACpRAABMogAAqEQB\nADCJAgCgEgUAwCQKAIBKFAAAkygAACpRAABMogAAqEQBADCJAgCgEgUAwCQKAIBKFFyXzp8/f9Rb\nuOGY+frMfH1mfv0TBdch/3DXZ+brM/P1mfn1TxQAAJUoAAAmUQAAVHXTlj/+iaq9vb0tL8N+ly5d\n6sKFC0e9jRuKma/PzNdn5uva97PzxFprjmVZtvfBx/i26jVbWwAArn93LMvy2jUW2nYUPLN6QfXe\n6sGtLQQA158T1WdVdy/L8sE1FtxqFAAA1w4nGgIAlSgAACZRAABUogAAmEQBAFBtOQrGGN85xrhv\njPGRMcZbxxjP2eZ616sxxivHGA9fcfmrK+7zg2OMD4wxPjzGeOMY4/Yrbv/4McZPjTEujjH+bYzx\nG2OMT133Mzm+xhjPHWO8bozx/jnfFz3KfZ7wjMcYnzzGeM0Y49IY40NjjJ8bYzx125/fcfSxZj7G\n+IVHOe5ff8V9zPwqjDG+b4zxtjHGv44xHhhj/PYY43Mf5X6O9UPy/5n5cTrWtxYFY4xvrV5VvbJ6\nVvXO6u4xxs621rzOvau6pbp1Xr7q8g1jjO+tvqv6jupLqn9vM+un7Hv/H6u+ofqW6qurT6t+c5Wd\nXxueWr2jenn1Ua/TPcQZv7barZ4/7/vV1c8c5idyDXncmU+/2yOP+zNX3G7mV+e51U9UX1p9ffXk\n6vfGGJ9w+Q6O9UP3MWc+HY9jfVmWrVyqt1Y/vu/tUf1D9YptrXm9XtqE1YXHuf0D1Z373n5a9ZHq\nxfve/o/qm/fd5/Oqh6svOerP77hd5lxedNgznv9YH66ete8+L6j+u7r1qD/vYzjzX6h+63Hex8yf\n+Nx35ny+at91jvX1Z35sjvWtPFIwxnhydbp68+Xrls0O31R9+TbWvAF8znyY9T1jjHNjjM+oGmPc\n1qYq98/6X6s/6/9m/ew2f+di/33urd6Xr8fHdIgz/rLqQ8uy/Pm+D/+mNr8lf+m29n+Ne958yPXd\nY4y7xhjP2Hfb6cz8ifqkNrP4l3Ksr+QRM9/nWBzr23r6YKd6UvXAFdc/0OaA4+q8tfr2NtX3suq2\n6g/nc0W3tvmiP96sb6n+c/7jfqz78NgOa8a3Vv+0/8ZlWR5q883B1+Gj/W71kurrqldUX1O9fowx\n5u23ZuYHNuf4Y9UfL8ty+Rwlx/oWPcbM6xgd69v+K4kcgmVZ7t735rvGGG+r/q56cfXuo9kVbNey\nLL+2782/HGP8RfWe6nnV7x/Jpq4vd1VfUH3lUW/kBvKoMz9Ox/q2Him4WD3Upij3u6W6f0tr3jCW\nZblU/XV1e5t5jh5/1vdXTxljPO1x7sNjO6wZ319debbwk6pn5OvwMS3Lcl+b7y2Xz4Q38wMaY/xk\n9cLqecuy/OO+mxzrW/I4M/8oR3msbyUKlmX5r+qeNmdAVv/7sMnzqz/dxpo3kjHGJ7Y5WD4wD577\ne+Ssn9bmOaTLs76nzckm++/zedXJ6i0rbfuadYgzfkv1SWOMZ+378M9v8034z7a1/+vFGOPTq2dW\nl7+hmvkBzB9O31R97bIs79t/m2N9Ox5v5o9x/6M71rd4huWLqw+3eZ7k89u8LOKD1acc9dmf19ql\n+pE2Ly35zOorqje2eS7pmfP2V8zZfmP1hdXvVH9TPWXfx7iruq/Nw1Gnqz+p/uioP7fjcmnz8rgv\nqr64zRm83z3f/ozDnHH1+urt1XPaPIR4b/UrR/35H7eZz9t+uM0Po8+c39zeXu1VTzbzA8/8rupD\nbV4md8u+y4l993Gsrzjz43asb3sYL6/e2+blLG+pnn3UX6Br8VKdb/Nyzo+0Odv0tdVtV9znB9q8\nlOjD1d3V7Vfc/vFtXit7sfq36terTz3qz+24XNqc2PNwm6e99l9+/jBn3ObM43PVpfmN4merm4/6\n8z9uM2/zd+Tf0Oa31gerv61+uit+qTDzq575o837oeolV9zPsb7SzI/bsT7mBwIAbnD+9gEAUIkC\nAGASBQBAJQoAgEkUAACVKAAAJlEAAFSiAACYRAEAUIkCAGASBQBAVf8DdL2Ov5X5FJcAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110e4a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_counts = df_train[\"Insult\"].value_counts(ascending=True)\n",
    "category_counts.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove_twitter_200d.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-76df7a079027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInsult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    732\u001b[0m             delayed(_fit_transform_one)(trans, name, weight, X, y,\n\u001b[1;32m    733\u001b[0m                                         **fit_params)\n\u001b[0;32m--> 734\u001b[0;31m             for name, trans, weight in self._iter())\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, name, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m                        **fit_params):\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rohitraghavan/anaconda/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-969e13216fa7>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, df, y)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#add both the features to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tweetVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scalar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-969e13216fa7>\u001b[0m in \u001b[0;36mmake_tweetVector\u001b[0;34m(self, df_comments)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_tweetVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlearned_word_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearned_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"glove_twitter_200d.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtweetVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove_twitter_200d.pkl'"
     ]
    }
   ],
   "source": [
    "best_clf.fit(df_train.Comment,df_train.Insult)\n",
    "predicted = best_clf.predict(df_test.Comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(df_test.Insult,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_labels = np.sort(df_train.Insult.unique())\n",
    "lables = [str(i) for i in class_labels]\n",
    "print(classification_report(df_test.Insult, predicted, target_names=lables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_clf.predict_proba([\"You are a nobody\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_clf.predict_proba([\"what the fuck is that\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train1, df_test1 = load_Dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions1 = best_clf.predict(df_test1.Comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(df_test1.Insult,predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(df_test1.Insult, predictions1, target_names=lables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, title, target_names, cmap=plt.cm.coolwarm):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_labels = np.sort(df_test1.Insult.unique())\n",
    "\n",
    "\n",
    "nb_cm = confusion_matrix(df_test1.Insult,predictions1)\n",
    "plot_confusion_matrix(nb_cm, \"SVM Confusion Matrix\", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the model a pickle for GUI use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#dup to pickle\n",
    "with open('model_demo1.pkl', 'wb') as f:\n",
    "    pickle.dump(best_clf, f)\n",
    "#Load the model from this pickle later\n",
    "best_clf = pickle.load(open('model_demo1.pkl','rb'))\n",
    "\n",
    "# returns confidence score ()\n",
    "def is_abuse(model,sentence):\n",
    "    confidence = model.predict_proba([sentence])\n",
    "    print(confidence)\n",
    "    return confidence\n",
    "\n",
    "\n",
    "sentence = \"you fuck!\"\n",
    "# calling the function\n",
    "is_abuse(best_clf,sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code used for hyperparameters, stratified K-fold and K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = df_train.Comment, df_test.Comment\n",
    "y_train,y_test = df_train.Insult, df_test.Insult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search to tune the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "best_clf_hyp = Pipeline([ ('vect', TfidfVectorizer(tokenizer=build_tokens,ngram_range=(1, 6),stop_words = stopwords)),\n",
    "                     ('clf', svm.SVC(decision_function_shape=\"ovo\",probability=True)), ])\n",
    "\n",
    "tuned_parameters = [{'clf__kernel': ['rbf'], 'clf__gamma': [1e-3, 1e-4],\n",
    "                     'clf__C': [1, 10, 100, 1000]},\n",
    "                    {'clf__kernel': ['linear'], 'clf__C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(best_clf_hyp, tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K fold cross validation - Did not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"train.csv\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "accuracy = 0\n",
    "best_model = None\n",
    "prediction = None\n",
    "y = None\n",
    "for train_index, test_index in skf.split(data_set.Comment,data_set.Insult):\n",
    "    X_train, X_test = data_set.Comment[train_index], data_set.Comment[test_index]\n",
    "    y_train,y_test = data_set.Insult[train_index], data_set.Insult[test_index]\n",
    "    # Drop any resulting nan values\n",
    "    X_train.dropna(inplace=True)\n",
    "    y_train.dropna(inplace=True)\n",
    "    X_test.dropna(inplace=True)\n",
    "    y_test.dropna(inplace=True)\n",
    "    # Train model using created datasets\n",
    "    _ = best_clf.fit(X_train, y_train)\n",
    "    print('Trained model on new train dataset.')\n",
    "    # Predict on test\n",
    "    predicted = best_clf.predict(X_test)\n",
    "    print('Accuracy on test dataset: ')\n",
    "    test_accuracy = accuracy_score(y_test,predicted)\n",
    "    print(test_accuracy)\n",
    "    \n",
    "    # Save models and accuracy so that we can use it later\n",
    "    if (test_accuracy > accuracy):\n",
    "        accuracy = test_accuracy\n",
    "        best_model = best_clf\n",
    "        prediction = predicted\n",
    "        y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y, prediction, target_names=lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions1 = best_model.predict(df_test1.Comment)\n",
    "accuracy_score(df_test1.Insult,predictions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering - Did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering by clustering our dataset into 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "raw = data_set\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,6), max_df= 0.5, analyzer= \"word\", stop_words = stopwords, tokenizer= build_tokens)\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(raw.Comment) #fit the vectorizer to raw text\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit by kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#clusters = [5,10,15]\n",
    "clusters = [2]\n",
    "for num_clusters in clusters:\n",
    "\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    %time km.fit(tfidf_matrix)\n",
    "\n",
    "    kmeans_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    print(\"\\nNumber of Clusters: %d\" % num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        topic = \"\"\n",
    "        for ind in kmeans_centroids[i, :200]:\n",
    "            topic += terms[ind] + \" \"\n",
    "        print(\"Top %d: %s\" % (i+1,topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os  # for os.path.basename\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "MDS()\n",
    "\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = km.labels_.tolist()\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Not-abusive', \n",
    "                 1: 'Abusive', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 10)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='',ms=12, label=cluster_names[name],mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is no clear distinction between the 2 clusters and both the clusters contain insults indicating that the clustering was not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
