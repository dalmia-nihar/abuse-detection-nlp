{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk, re, string\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re, collections\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import words as w\n",
    "\n",
    "from nltk.corpus import *\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tokens(text):\n",
    "    tweetTokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tokens = tweetTokenizer.tokenize(text)\n",
    "    #tokens = preprocess(text, lowercase=True)\n",
    "    #tokens = [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "    #tokens= [nltk.PorterStemmer().stem(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditionalFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return (['percent_bad','vader_compound','num_words','vader_neg','vader_pos'])\n",
    "    \n",
    "    def num_bad(self, df):\n",
    "        #get number of words in each sentence\n",
    "        num_words = [len(word) for word in df]\n",
    "        \n",
    "        #get percent of abusive words in each sentence\n",
    "        with open(\"list_of_abuses.txt\", \"r\") as abuse_list:\n",
    "            abuses = abuse_list.read().split()\n",
    "            num_abuses = 0\n",
    "            for abuse in abuses:\n",
    "                num_abuses += 1\n",
    "            # number of badwords in list of abuses\n",
    "            num_bad = [np.sum([word.lower().count(abuse) for abuse in abuses])\n",
    "                                                for word in df]\n",
    "            norm_bad = np.array(num_bad) / np.array(num_words, dtype=np.float)\n",
    "        return norm_bad\n",
    "    \n",
    "    def num_words(self,df):\n",
    "        #get number of words in each sentence\n",
    "        num_words = [len(word) for word in df]\n",
    "        return num_words\n",
    "    \n",
    "    def vader_helper(self, df):\n",
    "        #vader analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_feature = []\n",
    "        for sentence in df:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            vader_feature.append(ss['compound'])\n",
    "        return vader_feature\n",
    "    \n",
    "    def vader_helper_neg(self, df):\n",
    "        #vader analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_feature = []\n",
    "        for sentence in df:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            vader_feature.append(ss['neg'])\n",
    "        return vader_feature\n",
    "    \n",
    "    def vader_helper_pos(self, df):\n",
    "        #vader analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        vader_feature = []\n",
    "        for sentence in df:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            vader_feature.append(ss['pos'])\n",
    "        return vader_feature\n",
    "    def transform(self, df, y=None):     \n",
    "        #add both the features to an array\n",
    "        X = np.array([self.num_bad(df), self.vader_helper(df),self.num_words(df),self.vader_helper_neg(df),self.vader_helper_pos(df)]).T\n",
    "        #X = np.array([self.num_bad(df),self.vader_helper(df)]).T\n",
    "        #X.reshape(-1, 1) #use if only 1 feature\n",
    "        if not hasattr(self, 'scalar'):\n",
    "            self.scalar = preprocessing.StandardScaler().fit(X)\n",
    "        return self.scalar.transform(X)      \n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['a','an','the','or','is','are','was','were','have']\n",
    "def all_features():\n",
    "    features = []\n",
    "    custom_features = AdditionalFeatureExtractor() # this class includes my custom features \n",
    "    vect = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,6), analyzer= \"char\", stop_words = stopwords, tokenizer= build_tokens)\n",
    "    vect1 = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,6), analyzer= \"word\", stop_words = stopwords, tokenizer= build_tokens)\n",
    "    \n",
    "    features.append(('ngram', vect))\n",
    "    features.append(('ngram1', vect1))\n",
    "   \n",
    "    features.append(('custom_features', custom_features))\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version pre-0.18 when using version 0.18. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version pre-0.18 when using version 0.18. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version pre-0.18 when using version 0.18. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator FeatureUnion from version pre-0.18 when using version 0.18. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator SVC from version pre-0.18 when using version 0.18. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/nihar/anaconda3/envs/Info256/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version pre-0.18 when using version 0.18. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00297305  0.99702695]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00297305,  0.99702695]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "#Load the model from this pickle later\n",
    "best_clf = pickle.load(open('model.pkl','rb'))\n",
    "\n",
    "# returns confidence score ()\n",
    "def is_abuse(model,sentence):\n",
    "    confidence = model.predict_proba([sentence])\n",
    "    print(confidence)\n",
    "    return confidence\n",
    "\n",
    "\n",
    "sentence = \"you fuck!\"\n",
    "# calling the function\n",
    "is_abuse(best_clf,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Info256]",
   "language": "python",
   "name": "Python [Info256]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
